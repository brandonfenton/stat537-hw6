---
title: 'Stat 537: Homework 6'
author: "Brandon Fenton and Kenny Flagg"
date: "Due Tuesday, March 8 at 5:00 PM"
output: pdf_document
header-includes: \usepackage{float}
---


```{r setup, echo=F, message=F, warning=F}
require(pander)
```

The following will involve working with a data set related to spatial variation in a suite of potential predictor variables and then, eventually, for building a predictive model for the presence/absence of whitebark pine in the greater Yellowstone Ecosystem. 

For this work, we will focus on the historic climate and water balance data only (read the related sections carefully for variable names and definitions) in \url{http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0111669}.

Read the article as we will find all of the paper interesting before the end of the semester. Initially, we are interested in doing a PCA of the monthly 1950 to 1980 average minimum and maximum temperature, precipitation, and snow pack (Q=48). Note that the number of each variable is the month of the year from January to December (1 to 12). You can use tc1_r below for this first analysis as I subset the entire data set for you. 

The provided code will source in a modified version of corrplot.mixed that I will discuss in class. The short version is that it orders the variables based on a hierarchical cluster analysis using a dissimilarity measure that treats positive and negative correlations equally (two variables that have r=0.5 are just as similar as two variables that have r=-0.5). 

```{r p0_a, echo=F, warning=F, message=F, cache=T, fig.height=10, fig.width=10}
# Whatever R setup is on Euclid refuses to use https
tc1<-read.csv("tcdata.csv",header=T)
tc1$responsef<-factor(tc1$response)
tc1_r<-tc1[,c(4:39,64:75)]
cor1<-cor(tc1_r)
```

1) _Discuss the pattern in the correlation matrix._

There are four apparent clusters of variables. The first consists of the the precipitation and snowpack levels for November through March, as well as the snowpack level for April. These are all very highly correlated because the amount of precipitation in one month affects the amount of snowpack for that month, and then the snowpack carries over to the next month.

The second cluster includes all of the temperature variables. These have positive correlations since temperature trends continue from one month to the next. The minimum and maximum temperatures form two sub-clusters, with maxima being strongly correlated with other maxima, and minima strongly correlated with other minima, but the correlations are weak to moderate between minimum temperatures and maximum tempteratures. The temperature variables are negatively correlated with precipitation and snowpack.

The third cluster includes the rest of the precipitation and snowpack levels except for the snowpack in July and August. These have weak to moderate positive associations with each other and weak correlations with precipitation and snowpack in the windter months. The exceptions are snowpack for May and October, and precipitation for April and October, which are moderately correlated with the winter snowpack and precipitation variables because of the seasonal trends.

The final cluster is comprised only of snowpack for July and August, the months when there is very little snow. These are weakly correlated with June snowpack and essentially uncorrelated with all of the other variables.

```{r p1_a, echo=F, warning=F, message=F, cache=T, fig.height=8, fig.width=8}
require(corrplot)
source("corplotMG.R")
corrplot_mg(cor1,order="hclust",tl.pos="lt")
```

2) _Perform a PCA of these variables based on the correlation matrix, report a biplot and scree plot. No discussion, just plots._
```{r p2_a, echo=F, comment=NA, fig.height=3, fig.pos="H", fig.align="center", cache=T, size="sripctsize"}
layout(rbind(c(1,2,2)))

# PCA
pcs <- prcomp(tc1_r, scale=T, center=T)

# Biplot
biplot(pcs, col =  c("#00000020", "#ff000040"))

# Scree plot
plot(pcs, type="lines", main = "Eigenvalues")
abline(h=c(1, 0.75), lty=2:3)
```

3) _Interpret the first and fourth PCs based on the eigenvector coefficients._

```{r p3_a, echo=F, message=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="tiny"}

pander(pcs$rotation[,c(1,4)], caption = "The first and fourth principal components")
```

4) _Calculate the fourth PC using the predict function. Then replicate that calculation using the eigenvector and original variables (remember that the variables need to be standardized - the scale() function is a nice option). Show that they are the same._

The table compares the the predicted summer snowpack values for 20 observations found both using the predict function and by multiplying the data matrix by the principal component vector.

```{r p4_a, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
predict.scores <- predict(pcs)[,4]
ev.4 <- pcs$rotation[,4]
ev.scores <- as.matrix(scale(tc1_r)) %*% ev.4

# Take a random sample of predicted scores
set.seed(832)
rows <- sample(length(predict.scores), 20)
pander(cbind("predict()" = predict.scores[rows], "computed" = ev.scores[rows], "difference" = predict.scores[rows] - ev.scores[rows,]),
       caption = paste("Sum of all", length(predict.scores), "differences =", round(sum(predict.scores - ev.scores), 10)))
```

5) _Now use your interpretation of PC4 to define a set of coefficients that should involve a reduced set of coefficients that are "different" from 0 to calculate the PC 4 scores. Make a plot of the real scores using all coefficients and based on this subset and compare the results._

```{r p5_a, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
diff.0 <- which(ev.4 > 0.30)
ev.scores.red <- as.matrix(scale(tc1_r[,diff.0])) %*% ev.4[diff.0] 

plot(x=ev.scores.red, y = ev.scores, col=rgb(.6,0,0, .5), pch = 20,
     xlab = "Reduced PC4 (summer snowpack only)", ylab = "Full PC4")
abline(a=0, b=1)

```

6) _For the moment, we will focus on just January minimum temperatures (something they used as an explanatory variable in their predictive model). The following fits a bivariate tensor-product penalized regression spline as function of the latitude and longitude of the observations and generates an estimated surface for the mean temperature as a deviation from the mean. Does location seem to matter for the temperatures? (I am not expecting you to know anything about the GAM I am using - it is just an estimate of the mean temperature surface.)_

```{r, warning=F,message=F}
require(mgcv)
gm1<-gam(tmin1~te(lon,lat),data=tc1)
 plot(gm1)
 vis.gam(gm1)
``` 

There does seem to be a clear relationship between location and January minimum temperatures.  If this were not the case, we would expect to see a far simpler temperature surface since the GAM would essentially be fitting to noise.  If we shuffle the latitude/longitude pairs and refit the GAM, the resulting fit supports our expectation:

```{r p6_a, warning=F,message=F}
require(permute)
 
set.seed(4)

loc.shuffle <- shuffle(length(tc1$lon))
rlat <- tc1$lat[loc.shuffle]
rlon <- tc1$lon[loc.shuffle]
 
gm2<-gam(tmin1~te(rlon,rlat),data=tc1)

plot(gm2)
vis.gam(gm2)

gam.comparison <- rbind(summary(gm1)$s.table, summary(gm2)$s.table)

rownames(gam.comparison) <- c("Original", "Shuffled")
```

The approximate F-test for the GAM fit using the original spatial locations has an extremely low p-value, while the fit using shuffled locations is 0.46, further supporting the existence of a potential relationship between location and temperature:   

```{r p6_b, warning=F,message=F}
pander(gam.comparison, caption = "GAMs fit with original vs. shuffled spatial locations")
```  

This comparison has been made on the basis of a single permutation, so cannot be used to make any strong conclusions.  The Mantel test in question 7 provides a more thorough permutation-based comparison between temperature and distance.


7) _Perform a Mantel test for a Euclidean distance matrix between the tmin1's vs a Euclidean distance matrix between the spatial locations defined by the lat and lon variables. Report the null hypothesis for the test specific to the situation. And report what you can conclude based on the result. [Note: this may take a while to run on your computer and might! cause you to run out of RAM. You are welcome to work with other students to obtain a computer with sufficient resources to complete the permutations.] Does this result agree or disagree with your previous result._

```{r p7_a, echo=F, comment=NA, fig.pos="H", fig.align="center", cache=T, size="footnotesize"}
 require(vegan)
 spat.dists <- dist(data.frame(tc1$lat, tc1$lon))
 tmin1.dists <- dist(tc1_r$tmin1)
 
 # ptm <- proc.time()
 
 mantel(spat.dists, tmin1.dists)
 
 # exectime <- proc.time() - ptm
 # print(exectime)
```

# R Code Appendix:

Loading Data:
```{r a0, ref.label='p0_a', eval=F}
```

Problem 1:
```{r a1, ref.label='p1_a', eval=F}
```

Problem 2:
```{r a2, ref.label='p2_a', eval=F}
```

Problem 3:
```{r a3, ref.label='p3_a', eval=F}
```

Problem 4:
```{r a4, ref.label='p4_a', eval=F}
```

Problem 5:
```{r a5, ref.label='p5_a', eval=F}
```

Problem 6:
```{r a6, ref.label='p5_a', eval=F}
```

```{r a6b, ref.label='p5_b', eval=F}
```

Problem 7:
```{r a7, ref.label='p7_a', eval=F}
```

<!--- ### About This Markdown File

  * File creation date: `r Sys.Date()`
  * `r R.version.string`
  * R version (short form): `r getRversion()`
  * Additional session information
  
```{r echo=FALSE}
sessionInfo()  # could use devtools::session_info() if you prefer that
```
-->



